import feedparser
import os
import sys
from datetime import datetime, timezone, timedelta
import xml.etree.ElementTree as ET
from xml.dom import minidom

# -----------------------------
# CONFIGURATION
# -----------------------------
FEEDS = [
    "https://thediplomat.com/feed/",
    "https://www.foreignaffairs.com/rss.xml",
    "https://foreignpolicy.com/feed/",
    "https://evilgodfahim.github.io/ps/combined.xml",
    "https://evilgodfahim.github.io/eco/combined.xml",
    "https://www.eiu.com/n/feed/"
]

MASTER_FILE = "feed_master.xml"
DAILY_FILE = "daily_feed.xml"
LAST_SEEN_FILE = "last_seen.json"

MAX_ITEMS = 500
BD_OFFSET = 6  # Bangladesh UTC offset

import json

# -----------------------------
# UTILITIES
# -----------------------------
def parse_date(entry):
    """Try multiple date fields from feedparser entry"""
    date_fields = ["published_parsed", "updated_parsed", "created_parsed"]
    for field in date_fields:
        t = getattr(entry, field, None)
        if t:
            return datetime(*t[:6], tzinfo=timezone.utc)
    return datetime.now(timezone.utc)

def load_existing(file_path):
    """Load existing XML items"""
    if not os.path.exists(file_path):
        return []
    tree = ET.parse(file_path)
    root = tree.getroot()
    items = []
    for item in root.findall(".//item"):
        try:
            title = item.find("title").text or ""
            link = item.find("link").text or ""
            desc = item.find("description").text or ""
            pubDate = item.find("pubDate").text or ""
            pubDate_dt = datetime.strptime(pubDate, "%a, %d %b %Y %H:%M:%S %z")
            items.append({"title": title, "link": link, "description": desc, "pubDate": pubDate_dt})
        except:
            continue
    return items

def write_rss(items, file_path, title="Feed"):
    """Write items to RSS XML"""
    rss = ET.Element("rss", version="2.0")
    channel = ET.SubElement(rss, "channel")
    ET.SubElement(channel, "title").text = title
    ET.SubElement(channel, "link").text = "https://evilgodfahim.github.io/"
    ET.SubElement(channel, "description").text = f"{title} generated by script"

    for item in items:
        it = ET.SubElement(channel, "item")
        ET.SubElement(it, "title").text = item["title"]
        ET.SubElement(it, "link").text = item["link"]
        ET.SubElement(it, "description").text = item["description"]
        ET.SubElement(it, "pubDate").text = item["pubDate"].strftime("%a, %d %b %Y %H:%M:%S %z")

    # Pretty print
    xml_str = minidom.parseString(ET.tostring(rss)).toprettyxml(indent="  ")
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(xml_str)

# -----------------------------
# MASTER FEED UPDATE
# -----------------------------
def update_master():
    print("[Updating feed_master.xml]")
    existing = load_existing(MASTER_FILE)
    existing_links = {x["link"] for x in existing}
    new_items = []

    for url in FEEDS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                link = getattr(entry, "link", "")
                if link and link not in existing_links:
                    new_items.append({
                        "title": getattr(entry, "title", "No Title"),
                        "link": link,
                        "description": getattr(entry, "summary", ""),
                        "pubDate": parse_date(entry)
                    })
        except Exception as e:
            print(f"Error parsing {url}: {e}")

    all_items = existing + new_items
    all_items.sort(key=lambda x: x["pubDate"], reverse=True)
    all_items = all_items[:MAX_ITEMS]

    # Ensure at least one dummy item if empty
    if not all_items:
        all_items = [{
            "title": "No articles yet",
            "link": "https://evilgodfahim.github.io/",
            "description": "Master feed will populate after first successful fetch.",
            "pubDate": datetime.now(timezone.utc)
        }]

    write_rss(all_items, MASTER_FILE, title="Master Feed (Updated every 30 mins)")
    print(f"✅ feed_master.xml updated with {len(all_items)} items")

# -----------------------------
# DAILY FEED UPDATE
# -----------------------------
def update_daily():
    print("[Updating daily_feed.xml]")
    from_zone = timezone.utc
    to_zone = timezone(timedelta(hours=BD_OFFSET))

    # Load last seen
    if os.path.exists(LAST_SEEN_FILE):
        with open(LAST_SEEN_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            last_seen = data.get("last_seen")
            if last_seen:
                last_seen_dt = datetime.fromisoformat(last_seen)
            else:
                last_seen_dt = None
    else:
        last_seen_dt = None

    master_items = load_existing(MASTER_FILE)
    new_items = []
    for item in master_items:
        pub = item["pubDate"].astimezone(to_zone)
        if not last_seen_dt or pub > last_seen_dt:
            new_items.append(item)

    if not new_items:
        new_items = [{
            "title": "No new articles today",
            "link": "https://evilgodfahim.github.io/",
            "description": "Daily feed will populate after first articles appear.",
            "pubDate": datetime.now(timezone.utc)
        }]

    write_rss(new_items, DAILY_FILE, title="Daily Feed (Updated 9 AM BD)")

    # Save last seen
    last_dt = max([i["pubDate"] for i in new_items])
    with open(LAST_SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump({"last_seen": last_dt.isoformat()}, f)

    print(f"✅ daily_feed.xml updated with {len(new_items)} items")

# -----------------------------
# MAIN
# -----------------------------
if __name__ == "__main__":
    args = sys.argv[1:]
    if "--master-only" in args:
        update_master()
    elif "--daily-only" in args:
        update_daily()
    else:
        update_master()
        update_daily()